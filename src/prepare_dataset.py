import random
import os
from dotenv import load_dotenv
import json
from json.decoder import JSONDecodeError
import pandas as pd
from typing import Optional, Dict
from rag_langchain import qa_chain
from utils import PDF_PATH, TXT_PATH, ENV_PATH, CSV_PATH, FINE_TUNE_FILE_PATH, TRAIN_PATH, VALID_PATH
from utils import get_pdf_text, call_openai
from prompts import DEFAULT_GEN_DATA, VALIDATE_PROMPT, ROLE_PROMPT


# Load keys from environment
_ = load_dotenv(ENV_PATH)
OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]
OPENAI_MODEL_NAME = os.environ["OPENAI_MODEL_NAME"]


def formatting_data(text: str) -> Optional[Dict]:
    # Remove special token
    text = text.replace("```json", "")
    text = text.replace("```", "")
    # Convert to Json
    try:
        result = json.loads(text)
        return result
    except JSONDecodeError:
        return None


def generate_question_answer_data(text_file_path: str, save_file_path: str) -> bool:
    """
    Generate question answer pair for fine-tune using openai
    """
    # Read text file
    with open(text_file_path, "r") as f:
        content = f.read()
    # List topic in paper
    topics = ["ABSTRACT", "1 INTRODUCTION", "2 RELATED WORK", "3 GENERATIVE AGENT BEHAVIOR AND INTERACTION",
              "4 GENERATIVE AGENT ARCHITECTURE", "5 SANDBOX ENVIRONMENT IMPLEMENTATION", "6 CONTROLLED EVALUATION",
              "7 END-TO-END EVALUATION", "8 DISCUSSION", "9 CONCLUSION", "REFERENCES",
              "A ARCHITECTURE OPTIMIZATIONS", "B AGENT INTERVIEW QUESTIONS"]

    questions, answers, categories = [], [], []
    for i in range(len(topics)):
        j = i+1
        # Find start and end index of each section
        start_idx = content.find(topics[i])
        end_idx = content.find(topics[j]) if j < len(topics) else len(content)
        # Get content and title of section
        section_content = content[start_idx:end_idx]
        section_title = topics[i]
        # Skip REFERENCES section
        if section_title != "REFERENCES":
            # Call openai api
            section_prompt = f"\nHere is {section_title} section: \n{section_content}"
            qa_data = call_openai(DEFAULT_GEN_DATA, section_prompt)
            # Processing the result
            qa_data = formatting_data(qa_data)
            # Extend to list
            questions.extend(qa_data['questions'])
            answers.extend(qa_data['answers'])
            categories.extend([section_title]*len(qa_data['questions']))
            assert len(questions) == len(answers)
    # Create dataframe to store raw data
    df = pd.DataFrame(columns=['Question', 'Answer', 'Context', 'Category', 'Verify'])
    df['Question'] = questions
    df['Answer'] = answers
    df['Category'] = categories
    # Save to csv file
    df.to_csv(save_file_path, encoding='utf-8', index=False)

    return True


def generate_context_data(file_path: str, save_file_path: str) -> bool:
    """
    Using langchain RAG to search context(answer) in paper,
    then verify the answer generated by openai is True or False
    """
    # Read csv file
    df = pd.read_csv(file_path)
    contexts, verify = [], []
    for index, row in df.iterrows():
        # Find context in paper via langchain RAG
        context = qa_chain({'question': row['Question']})['answer']
        # Create prompt to verify the answer
        prompt = f"\nHere is context:\n{context}\nHere is question:\n{row['Question']}\nHere is answer:\n{row['Answer']}"
        verify_result = call_openai(VALIDATE_PROMPT, prompt)
        contexts.append(context)
        verify.append(verify_result)
    # Store to dataframe
    df['Context'] = contexts
    df['Verify'] = verify
    df.to_csv(save_file_path, encoding='utf-8', index=False)

    return True


def review_and_correct_data(file_path: str, save_file_path: str) -> bool:
    """
    Review the dataset, remove and correct the False answer
    """
    # Read csv file
    df = pd.read_csv(file_path)
    # Check True and False answer
    count = df['Verify'].value_counts()
    print(count)
    # Correct the False answer
    indx = [40, 46, 48, 56, 57, 58, 61, 64, 70, 79, 88, 92, 97, 99, 100, 102]
    df.loc[indx, 'Answer'] = df.loc[indx, "Context"].to_list()
    df.loc[indx, 'Verify'] = True
    # Create new dataset with 100 row question-answer data
    true_df = df[df.Verify == True]
    true_df.to_csv(save_file_path, encoding='utf-8', index=False)

    return True


def processing_fine_tune_data(file_path: str, role_prompt: str,
                              train_file_path: str, valid_file_path: str) -> True:
    # Read csv file
    df = pd.read_csv(file_path)
    # Convert to openai fine-tune format
    data_formatted = [
        {"messages": [
            {"role": "system", "content": role_prompt},
            {"role": "user", "content": row["Question"]},
            {"role": "assistant", "content": row["Answer"]}]} for i, row in df.iterrows()]
    # Shuffle data
    random.shuffle(data_formatted)
    # Create train and validate set
    train_data = data_formatted[:80]
    valid_data = data_formatted[80:]
    # Write to jsonl file
    with open(train_file_path, "w") as f:
        for line in train_data:
            json.dump(line, f)
            f.write("\n")
    with open(valid_file_path, "w") as f:
        for line in valid_data:
            json.dump(line, f)
            f.write("\n")

    return True


if __name__ == "__main__":

    # Get text from pdf
    pdf_text = get_pdf_text(PDF_PATH)
    # Write to txt file
    with open(TXT_PATH, 'w') as f:
        f.write(pdf_text)
    # Generate data
    generate_question_answer_data(TXT_PATH, CSV_PATH)
    # Generate context and verify the answer
    generate_context_data(CSV_PATH, CSV_PATH)
    # Review data
    review_and_correct_data(CSV_PATH, FINE_TUNE_FILE_PATH)
    # Create train and valid data
    processing_fine_tune_data(FINE_TUNE_FILE_PATH, ROLE_PROMPT, TRAIN_PATH, VALID_PATH)
